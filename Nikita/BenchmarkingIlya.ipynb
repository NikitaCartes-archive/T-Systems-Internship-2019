{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import skimage\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'C:/Users/Nikita/Desktop/T-Systems/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_MEAN = np.load('x_mean.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, size_for_control):\n",
    "    X = []\n",
    "    y = np.concatenate([np.tile(row, (size_for_control, 1)) for row in np.eye(6)])\n",
    "    for c in os.listdir(path):\n",
    "        for image in os.listdir(path + '/' + c)[:size_for_control]:\n",
    "            try:\n",
    "                file_path = path + '/' + c + '/' + image\n",
    "                img = Image.open(file_path).convert('L')\n",
    "                img_data = np.asarray(img, dtype=np.int16)\n",
    "                X.append(img_data)\n",
    "            except:\n",
    "                print('Error: ' + file_path)\n",
    "    X = np.asarray(X)\n",
    "    return X.reshape(len(X), *X[0].shape, 1), y\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    idx = list(range(len(X)))\n",
    "    np.random.shuffle(idx)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def preprocess(X):\n",
    "    return (X.astype('float32')) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 7500\n",
    "X, y = load_data(DATA_PATH + 'TrainGray', train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000, 100, 150), (45000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(data_directory):\n",
    "    directories = [d for d in os.listdir(data_directory) \n",
    "                   if os.path.isdir(os.path.join(data_directory, d))]\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in directories:\n",
    "        label_directory = os.path.join(data_directory, d)\n",
    "        file_names = [os.path.join(label_directory, f) \n",
    "                      for f in os.listdir(label_directory) \n",
    "                      if f.endswith(\".png\")]\n",
    "        for f in file_names:\n",
    "            images.append(skimage.data.imread(f))\n",
    "            labels.append(d)\n",
    "    for i in range(len(images)):\n",
    "        images[i] = images[i]/255\n",
    "    c = list(zip(images, labels))\n",
    "    random.shuffle(c)\n",
    "    images, labels = zip(*c)\n",
    "    labels= [int(i) for i in labels]\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "ROOT_PATH = \"C:/Users/Nikita/Desktop/T-Systems\"\n",
    "train_data_directory = os.path.join(ROOT_PATH, \"TrainGray\")\n",
    "images, labels = load_data(train_data_directory)\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(t):\n",
    "    \n",
    "    images_test = images[t*9000:(t+1)*9000]\n",
    "    labels_test = labels[t*9000:(t+1)*9000]\n",
    "    \n",
    "    #images_test = np.array(images_test)\n",
    "    #labels_test = np.array(labels_test)\n",
    "    \n",
    "    images_train = np.concatenate((images[0:t*9000], images[(t+1)*9000:]))\n",
    "    labels_train = np.concatenate((labels[0:t*9000], labels[(t+1)*9000:]))\n",
    "    \n",
    "    return images_train, labels_train, images_test, labels_test\n",
    "\n",
    "class_names = ['button', 'checkbox', 'radiobutton', 'slider', 'spinner', 'textfield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 16:34:26.579456  6064 deprecation.py:323] From c:\\users\\nikita\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36000 samples\n",
      "Epoch 1/30\n",
      "36000/36000 [==============================] - 35s 986us/sample - loss: 1.7906 - accuracy: 0.1767\n",
      "Epoch 2/30\n",
      "36000/36000 [==============================] - 32s 891us/sample - loss: 1.7883 - accuracy: 0.1842\n",
      "Epoch 3/30\n",
      "36000/36000 [==============================] - 32s 895us/sample - loss: 1.7847 - accuracy: 0.1781\n",
      "Epoch 4/30\n",
      "36000/36000 [==============================] - 33s 909us/sample - loss: 1.7792 - accuracy: 0.1745\n",
      "Epoch 5/30\n",
      "36000/36000 [==============================] - 33s 916us/sample - loss: 1.7719 - accuracy: 0.1695\n",
      "Epoch 6/30\n",
      "36000/36000 [==============================] - 34s 939us/sample - loss: 1.7652 - accuracy: 0.1690 29s - loss: 1 - ETA: 27s - loss: 1.7640 - accuracy: - ETA: 27s - loss: 1.7643 - accuracy: 0.17 - ETA: 27s - loss: 1.764 - ETA: 26s - loss: 1.7657 - accuracy: 0.1 - ETA: 25s - loss: 1.7656 - accuracy: 0.172 - ETA: 25s  - ETA: 23s - loss: 1.7657 - - ETA: 22s - loss: 1.7660 - accuracy: 0.1 - ETA: 22s - loss: 1.7661 - accura - ETA:  - ETA: 19s  - ETA: 17s - loss: 1.7661 - - ETA: 16s  - ETA: 14s - lo - ETA: 12s - loss: 1.7670 - accura - ETA: 12s - loss: 1.7 - ETA: 11s - loss: 1.76 - E - ETA: 3s - los - ETA: 2s - E - ETA: 0s - loss: 1.7652 - accuracy\n",
      "Epoch 7/30\n",
      "36000/36000 [==============================] - 34s 935us/sample - loss: 1.7592 - accuracy: 0.17059s - loss: 1.7615 - accurac - ETA: 29s - loss: 1.7617 - - ETA: 28s - lo - ETA: 26s - loss: 1.760 - ETA: 19s - loss: 1.7611 - accuracy: 0.1 - ETA: 19s - loss: 1. - ETA: 18s - loss: 1.7620 - accuracy: 0.16 - ETA: 18s - - ETA: 16s - loss: 1.7613 - accura - ETA: 15s - loss: 1.7609 - accu - ETA: 14s -  - ETA: 13s - loss: 1.7610 - - ETA: 12s - loss: 1 - ETA: 10s - loss: 1.7595 - accuracy: - ETA: 10s - loss: 1.7593 - accuracy: 0. - ETA: 10s - loss: 1.7593 - accuracy: 0.172 -  - ETA: 2s - loss: 1.7593 - accuracy:  - ETA: 1s - ETA: 0s - loss: 1.7595 - ac - ETA: 0s - loss: 1.7594 \n",
      "Epoch 8/30\n",
      "36000/36000 [==============================] - 34s 936us/sample - loss: 1.7515 - accuracy: 0.17246s - loss: 1.7568 - ac - ETA: 25s - loss: 1.7554 - accuracy: 0.169 - ETA: 25s - loss: 1.7556 - accur - ETA: 24s - loss: 1.7557 - accuracy: - ETA: 24s - loss: 1.7557 - accu - ETA: 23s - loss: 1.7553 - accuracy: 0 - E - ETA: 20 - ETA: 18s - loss: 1.7528 - accur - ETA: 17s - loss: 1.7525 - accura - ETA: 14s - loss: 1.7533 - accu - ETA: 13s - loss: 1.7532 - accura - ETA: 13s - loss: 1.7530 - accuracy: 0.1 - ETA: 13s - loss: 1.7531 - accuracy: 0.172 - ETA: 12s - loss: 1.7531 - accuracy: 0.17 -  - ETA: - ETA: 7s - - ETA: 6s - loss: 1.7519 - accuracy:  - ETA: 6s - loss: 1.7518  - ETA: 6s - loss: 1.751 - ETA: 5s - l - ETA: 3s - loss: 1.7516 - accu - ETA: 2s - loss: 1.7514 - accu - - ETA: 1s - loss: 1.7513 - accuracy: 0.17 - ETA: 1s - loss: 1.7 - ETA: 0s - loss: 1.7514 \n",
      "Epoch 9/30\n",
      "36000/36000 [==============================] - 34s 943us/sample - loss: 1.7435 - accuracy: 0.1795- ETA: 30s - loss: 1.7447 -  - ETA: 29s -  - ETA: 27s - loss: 1.7441 - - ETA: 26s  - ETA:  - ETA: 17s - loss: 1.7454 - accuracy: - ETA: 16s - loss: 1.7455 - accuracy: 0.176 - ETA: 16s - l - - ETA: 12s - loss: 1.7452 - a - ETA: 11s - loss: 1.7451 - accura - ETA: 10s - loss:  - ETA: 5s - ETA: 4s - loss: 1.7449 - accuracy: 0.17 - ETA: 4s - loss: - ETA: 3s - loss: 1.7445 - ac - ETA: 3s - loss: 1.7445 - ac - ETA: 0s - loss: 1.7435 - accuracy\n",
      "Epoch 10/30\n",
      "36000/36000 [==============================] - 33s 903us/sample - loss: 1.7324 - accuracy: 0.1912ETA: 1s - l - ETA: 0s - loss: 1.7327 - accuracy\n",
      "Epoch 11/30\n",
      "36000/36000 [==============================] - 34s 934us/sample - loss: 1.7210 - accuracy: 0.20855s - loss: 1.7390  - ETA: 27s - loss:  - ETA: 2 - ETA: 20s - loss: 1.7252 - accura - ETA: 20s - loss: 1.7251 - accura - ETA: 19s - loss: 1.7246 - accuracy: 0.2 - ETA: 19s - l - ETA: - ETA: 15s - loss: 1.7236 - accuracy:  - ETA: 14 - ETA: 0s - loss: 1.7211 - accura - ETA: 0s - loss: 1.7212 - ac\n",
      "Epoch 12/30\n",
      "36000/36000 [==============================] - 34s 937us/sample - loss: 1.7050 - accuracy: 0.23550s - loss: 1.7188 -  - ETA: 26s - loss: 1.7148 - accuracy: 0.226 - ETA: 26s - loss:  - ETA: 25s - loss: 1.7142 - accuracy: 0. - ETA: 24s - loss: 1.7141 - accu - ETA: 24s - loss: 1.7128 - ac - ETA: 23s - loss: 1.7114 - accuracy: 0.2 - ETA: 14s - loss: 1.709 - ETA: 13s - loss: 1.7091 -  - ETA: 12s - los - ETA: 9s - loss: 1.7072 - accuracy:  - ETA: 9s - loss: 1.7074 - accura - ETA: 2s - loss: 1.7055  - ETA: 1s - loss: 1.7052  - ETA: 1s - loss: 1.7052 - accura - ETA: 0s - loss: 1.705 - ETA: 0s - loss: 1.7049 - accuracy: 0.\n",
      "Epoch 13/30\n",
      "36000/36000 [==============================] - 34s 938us/sample - loss: 1.6846 - accuracy: 0.26579s - loss: 1.6862 - accuracy: 0 - ETA: 29s - loss - ETA - ETA: 25s - loss:  - ETA: 23s - ETA: 21s - loss: 1.6893 -  - ETA: 20s - loss: 1.6891 - accuracy: 0. - ETA: 20s - loss:  - ETA: 19s - loss: 1.69 - ETA: 17s - loss: 1 - ETA: 16s - loss:  - ETA: 15s - loss: 1.6911 - accuracy: 0.256 - ETA: 14s - loss: 1.6910 - acc - ETA: 14s - lo - ETA: 12s - loss: 1.6903 - - ETA: 11s - loss: 1.6897 - accuracy:  - ETA: 11s - loss: 1.6898 - accuracy: - ETA: 10s - loss: 1.6896 - ac - ETA: 9s - loss: 1.6899 - accuracy: 0. - ETA: 9s - loss: 1.689 - ETA: 9s - loss: 1.6894 - accuracy: 0. - ETA - - ETA: 6s - loss: 1.6 - ETA: 6s - loss: 1 - ETA: 5s - loss: 1.6867  - ETA: 4s - loss: 1.6867 - accu - ETA: 4s - loss: 1.6867 - ac - - ETA: 2s - loss: 1.6861 - accura - ETA: 2s - loss: 1 - ETA: 1s - loss: 1.6861 - ac -\n",
      "Epoch 14/30\n",
      "36000/36000 [==============================] - 34s 941us/sample - loss: 1.6594 - accuracy: 0.29992s - loss: 1.6609 - accuracy:  - ETA: 32s - loss: 1.6624 - accu - ETA: 31s  - ETA: 2 - ETA: 27s - loss: 1 - ETA: 26s - loss: 1.6787 - accuracy: - ETA - ETA: 23s - loss: 1.6770 - accuracy: - ETA: 23s - loss: 1.6769 - accuracy: 0. - ETA: 20s - loss: 1.6726 - accu - ETA: 19s - loss: 1.67 - ETA: 18s - loss: 1.6695 - accuracy: 0. - ETA: 17s - loss: 1.6690 - accuracy: 0.292 - ETA: 17s - loss:  - ETA: 16s - loss: 1.6683 - acc - ETA: 15s - loss: 1.6683 - acc - ETA: 14s - loss: 1.6682 - accur - ETA - ETA: 11s - loss: 1.6658 - accu - ET - - ETA: 7s - - ETA: 6s - loss: - ETA: 2s - loss: 1.6608 - accuracy: 0.29 - ETA: 2s - loss: - ETA: 1s - loss: 1.6606 - accura - ETA: 1s - loss: - ETA: 0s - loss: 1.6597 - accura\n",
      "Epoch 15/30\n",
      "36000/36000 [==============================] - ETA: 0s - loss: 1.6268 - accuracy: 0.3313 - ETA: 28s - loss: 1.6372 - accurac - ETA: 25s - loss: 1.6364 - accuracy: 0 - ETA: 24s - loss: 1.6369 - accuracy: 0.3 - ETA: 24s - loss: 1.6372 - accu - ETA: 23 - ETA: 21s - loss: 1.6371 - accuracy: - ETA: 21s - loss: 1.6367 - accuracy: 0. - ETA: 21s - loss: 1.6363 - accuracy: - ETA: 20s - loss: 1.6372 - accuracy: 0.32 - ETA: 20s - loss: 1.6366 - accuracy: 0.323 - ETA: 20s - loss: 1.6366 - accuracy: 0 - ETA: 20s - loss: 1.6 - ETA: 16s - loss: 1.6351 - accuracy: - ETA: 15s - loss: 1.6352 - accuracy: - ETA: 15s - E - ETA: 10s - loss: 1.633 - ETA: 9s - loss: 1.6334 - accura - ETA: 9s - loss: 1.6337 -  - ETA: 9s - loss: 1.6329 - accura - ETA: 8s - loss: 1.632 - - ETA: 4s - loss: 1.6292 - accura - - ETA: 1s - l - ETA: 0s - loss: 1.6267 - ac - ETA: 0s - loss: 1.6268 - accuracy: 0.33 - 34s 940us/sample - loss: 1.6268 - accuracy: 0.3313\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/36000 [==============================] - 34s 938us/sample - loss: 1.5903 - accuracy: 0.35423s - loss: 1.6169 - accuracy: 0.338 - ETA: 33s - loss: - ETA: 31s - loss: 1.6153 - accuracy - ETA: 31s - loss: 1.6131 - accuracy: 0 - ETA: 30s - loss: 1.6121 - accur - ETA: 30s - loss: 1.614 - ETA: 23s - loss: 1.6093 -  - ETA: 22s - loss: 1.6084 - accuracy: - ETA: 22s - loss: 1.6078  - ETA: 21s - loss: 1.6068 - accuracy: 0. - ETA: 20s - lo - ETA: 19s - loss: 1.6030 - accuracy: 0. - ETA: - ETA: 16s - loss: 1.6028 - accuracy:  - ETA: 16s - loss: 1.6023 - accuracy: - ETA: 15s - loss: 1. - ETA: 14s - loss: 1.5983 - accuracy: - ETA: 13s - loss: 1.59 - ETA: 12s - loss: 1.5963 - accuracy: 0.354 - ETA: 12s - loss: 1.5962 - - ETA: 11s - loss: 1.5958 - accuracy: - ETA: 11s - loss: 1.5962 - ac - ETA: 10s - loss: 1.5956 - accu - ETA: 8s - loss: - ETA: 6s - loss: 1.5937 - accuracy: 0. - ETA: 6s - l - E - ETA - ETA: 1s - loss: 1.5911 -  - ETA\n",
      "Epoch 17/30\n",
      "36000/36000 [==============================] - 34s 940us/sample - loss: 1.5505 - accuracy: 0.37703s - loss: 1.5423 - accura - ETA: 32s - loss: 1.5427 - accur - ETA: 31s - loss: 1.5520 - accuracy: 0.3 - ETA: 31s - loss: 1.5548  - ETA: 30s - loss: 1.5568 - accuracy - ETA: 29s - loss:  - ETA: 28s - loss: 1.5616 - - ETA: 27s - loss: 1.5647 - accuracy: 0.375 - ETA: 27s - loss: 1.5644 - accu - ETA: 26s - loss: 1.5646 - accu - ETA: 25s - loss: 1.5653 - accuracy: - ETA: 25s - loss: 1.5642 - accuracy - ETA: 24s - loss: 1.5628 - accuracy: - ETA: 24s - loss: 1.5636 - accurac - ETA: 24s - loss: 1.5645 - accuracy: 0 - ETA: 23s - loss: 1.5654 - accur - ETA: 23s - loss: 1.5638 - accuracy: 0. - ETA: 22s - loss: 1.5628 - accuracy - ETA: 22s - loss: 1.5629 - ETA: 21s - loss: 1.5631 - accuracy: - ETA: 20s - loss - ETA: 19s - loss: 1.5623 - accuracy: 0.37 - ETA: 19s - loss: - ETA: 17s - loss: 1.5612 - accuracy - ETA: 16s - loss: 1.5607 - acc - ETA: 16s - loss: 1.5600 - a - ETA: 15s - loss: 1.5597 - accur - ETA: 14s - loss: 1.5 - ETA: 13s - loss: 1.5577 - accuracy: 0 - ETA: 12s - loss: 1.5578 - accuracy:  - ETA: 12s - loss: - E - - ETA: 6s - loss: 1.5 - ETA: 5s - loss: 1.5533 -  - ETA: 4s - loss: 1.552 - ETA: 3s - loss: - ETA: 1s - loss: 1.5510 - ac - ETA: 1s - loss: 1.5508 - accu - ETA: 0s - loss: 1.5\n",
      "Epoch 18/30\n",
      "36000/36000 [==============================] - 34s 937us/sample - loss: 1.5096 - accuracy: 0.39163s - loss: 1.5512 - accuracy: 0.358 - ETA: 33s - loss: 1.5491 - ac - ETA: 32s - loss: 1.5190 - accura - ETA: 32 - ETA:  - ETA: 27s - loss: 1.5280 -  - ETA: 26s - loss: 1.5295 - accuracy: - ETA: 26s - loss: 1.5296 - accuracy:  - ETA: 25s - loss: 1.5285 - a - ETA: 22s - loss: 1 - ETA: 20s - loss: 1.5244 - accuracy: 0.382 - ETA: 20s - loss: 1.5250 - accuracy: 0 - ETA: 20s - loss: 1.5249 - accuracy:  - ETA: 20s - loss: 1.5243 - accuracy: 0.381 - ETA: - ETA: 17s - loss: 1.5199 - accuracy: - ETA: 17s - loss: 1.5190 - accur - ETA: 16s - loss: 1.5187 - accu - ETA: 15s - loss: 1.5178 - - ETA: 12s - loss: 1.5143 - accura - ETA: 11s - loss: 1.5136 - - ETA: 10s - loss: 1.5132 - accuracy: 0 - ETA: 10s - loss: 1.5134 - accuracy - ETA - ETA: 8s - loss: 1.5131  - ETA: 6s - ETA: 5s - loss: 1.5118 - accuracy: 0. - ETA: 5s - loss: 1.5119 - accuracy - ETA: 5s - loss: - ETA: 0s - loss:\n",
      "Epoch 19/30\n",
      "36000/36000 [==============================] - 34s 940us/sample - loss: 1.4755 - accuracy: 0.40272s - loss: 1.494 - ETA: 31s - loss: 1.5039 - accuracy:  - ETA: 31s - loss: 1.5016 - acc - ETA: 30s - loss: 1.5007 - accuracy:  - ETA: 30s - loss: 1.4970 - accuracy: - ETA: 29s - loss - ETA: 28s - loss: 1.4882 - accuracy: 0.402 - ETA: 27s - loss: 1.4875 - accuracy - ETA: 27s - loss: 1.4885 - ETA: 26s - loss: 1.4891 - accuracy: 0.401 - ETA: 26s - lo - ETA: 24s - loss: 1.4887 - acc - ETA: 23s - loss:  -  - ETA: 19s - loss: 1.4861 - accuracy - ETA: 19s - loss: 1.4868 - accuracy:  - ETA: 18s - loss: 1.4866 - accuracy: 0.4 - ETA: 18s - loss: 1.48 - ETA: 17s - - ETA: 3s - loss: 1 - ETA: 1s - loss: 1.4763 - accuracy: 0. - ETA: 1s - loss: 1.4762 - accuracy: 0.40 - ETA: 1s - loss: 1.4761 - accura - ETA: 0s - loss: 1.4759 - accuracy: 0. - ETA: 0s - loss: 1.4757 -  - ETA: 0s - loss: 1.4753 - accuracy: 0.40 - ETA: 0s - loss: 1.4751 - accuracy: 0.\n",
      "Epoch 20/30\n",
      "36000/36000 [==============================] - 33s 924us/sample - loss: 1.4445 - accuracy: 0.41213s - loss: 1.4477 - accuracy:  - ETA: 33s - loss: 1.4250 - ETA: 31s - lo - ETA: - ETA: 28s - loss: 1.4444 - accuracy: 0.410 - ETA: 28s - loss: 1.4438 - accuracy: 0.41 - ETA: 28s - loss: 1.4457 - accuracy: 0.41 - ETA: 27s - loss: 1.4464 - accuracy: 0 - ETA: 27s \n",
      "Epoch 21/30\n",
      "36000/36000 [==============================] - 33s 905us/sample - loss: 1.4153 - accuracy: 0.4225- loss: 1.4158 - accura - ETA: 5s - loss: - ETA: 4s - loss: 1.4162 -  - E - ETA: 1s - loss: 1.4155  - ETA: 0s\n",
      "Epoch 22/30\n",
      "36000/36000 [==============================] - 33s 930us/sample - loss: 1.3887 - accuracy: 0.43133s - loss: 1. - ETA: 31s - loss: 1.4001 - accuracy: 0. - ETA: 31s  - ETA: 2 - ETA: 27s - loss: 1.3992 - accurac - ETA: 26s - loss: 1.39 - ETA: 25s - loss: 1. - ETA: 21s - loss: 1.3954 - ETA: 17s - loss: 1.3974 - accuracy: 0. - ETA: 17s - loss: 1.39 - ETA: 16s - loss: 1.3965 - accu - ETA: 15s - loss: 1.39 - ET - ETA: 1s - loss: 1 - ETA: 0s - loss: 1.3 - ETA: 0s - loss: 1.3888 - accuracy\n",
      "Epoch 23/30\n",
      "36000/36000 [==============================] - 33s 930us/sample - loss: 1.3659 - accuracy: 0.44312s - loss: 1.3417 -  - ETA: 31s - loss: 1.3443 - - ETA: 30s - ETA: 28s - loss: 1.3581 - accur - ETA: 27s - loss: 1.3579 - accura - ETA:  - ETA: 25s - loss: 1.3723 - accuracy: 0 - ETA: 24s - lo - ETA: 23s -  - ETA: 21s - loss: 1.3700 - accuracy - ETA: 12s - loss: 1.3686 -  - ETA: 11s - loss: 1.3696 - accuracy: 0.441 - ETA: 11s - loss: 1.3697 -  - ETA: 10s - loss: 1.3689 - accuracy: 0.442 - - ETA: 3s - l - ETA: 2s - loss: 1.3667 - accuracy - ETA: 2s - - ETA - ETA: 0s - loss: 1.365\n",
      "Epoch 24/30\n",
      "36000/36000 [==============================] - 34s 935us/sample - loss: 1.3457 - accuracy: 0.44675 - ETA: 23s - loss: 1.3522 - accuracy:  - ETA: 22s - loss: 1.3521 - accuracy: 0.444 - ETA: 22s - ETA: 20s - loss: 1.3460 - accuracy:  - ETA: 20s - loss: 1.3459 - accuracy:  - ETA: 20s - loss: 1.3452 - - ETA: 19s - loss: 1.3452 - accuracy: 0.449 - ETA: 1 - ETA: 16s - loss: 1.3446 - accu - ETA: 16s - loss: 1.3458 -  - ETA: - ETA: 2s - l\n",
      "Epoch 25/30\n",
      "36000/36000 [==============================] - 33s 915us/sample - loss: 1.3238 - accuracy: 0.4561\n",
      "Epoch 26/30\n",
      "36000/36000 [==============================] - 32s 900us/sample - loss: 1.3049 - accuracy: 0.4662\n",
      "Epoch 27/30\n",
      "36000/36000 [==============================] - 32s 892us/sample - loss: 1.2861 - accuracy: 0.4729\n",
      "Epoch 28/30\n",
      "36000/36000 [==============================] - 32s 891us/sample - loss: 1.2721 - accuracy: 0.4780\n",
      "Epoch 29/30\n",
      "36000/36000 [==============================] - 32s 892us/sample - loss: 1.2559 - accuracy: 0.4869\n",
      "Epoch 30/30\n",
      "36000/36000 [==============================] - 32s 891us/sample - loss: 1.2386 - accuracy: 0.4928\n",
      "9000/9000 [==============================] - 4s 485us/sample - loss: 1.1615 - accuracy: 0.5579 - - ETA: 0s - loss: 1.1\n",
      "Result: 0.55788887\n",
      "Train on 36000 samples\n",
      "Epoch 1/30\n",
      "36000/36000 [==============================] - 33s 903us/sample - loss: 1.7884 - accuracy: 0.1777\n",
      "Epoch 2/30\n",
      "36000/36000 [==============================] - 32s 890us/sample - loss: 1.7838 - accuracy: 0.1762\n",
      "Epoch 3/30\n",
      "36000/36000 [==============================] - 32s 891us/sample - loss: 1.7768 - accuracy: 0.1729\n",
      "Epoch 4/30\n",
      "36000/36000 [==============================] - 32s 891us/sample - loss: 1.7701 - accuracy: 0.1723- loss: 1.7702 - accuracy:  - ETA: 0s - loss: 1.7701 - accuracy\n",
      "Epoch 5/30\n",
      "36000/36000 [==============================] - 32s 895us/sample - loss: 1.7628 - accuracy: 0.1729\n",
      "Epoch 6/30\n",
      "36000/36000 [==============================] - 32s 891us/sample - loss: 1.7559 - accuracy: 0.1775\n",
      "Epoch 7/30\n",
      "31008/36000 [========================>.....] - ETA: 4s - loss: 1.7484 - accuracy: 0.1854"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-42a7589d3891>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mimages_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikita\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\nikita\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\nikita\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikita\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3510\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3512\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikita\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 572\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikita\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikita\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 445\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    446\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikita\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(input_shape=(100, 150, 1), kernel_size = (3, 3), filters = 32, activation='relu'),\n",
    "        keras.layers.MaxPool2D(pool_size = (2, 2)),\n",
    "        keras.layers.Conv2D(kernel_size = (3, 3), filters = 64, activation='relu'),\n",
    "        keras.layers.MaxPool2D(pool_size = (2, 2)),\n",
    "        keras.layers.Conv2D(kernel_size = (5, 5), filters = 128, activation='relu'),\n",
    "        keras.layers.Conv2D(kernel_size = (3, 3), filters = 64, activation='relu'),\n",
    "        keras.layers.MaxPool2D(pool_size = (2, 2)),\n",
    "        keras.layers.Conv2D(kernel_size = (5, 5), filters = 64, activation='relu'),\n",
    "        keras.layers.MaxPool2D(pool_size = (2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(512, activation='relu'),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Dense(6, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "              optimizer=tf.keras.optimizers.Adadelta(), \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    images_train, labels_train, images_test, labels_test = get_data(i)\n",
    "    \n",
    "    images_train = images_train.reshape(-1, 100, 150, 1)\n",
    "    images_test = images_test.reshape(-1, 100, 150, 1)\n",
    "    \n",
    "    model.fit(images_train, labels_train, epochs=30)\n",
    "    \n",
    "    loss, acc = model.evaluate(images_test, labels_test)\n",
    "    print(\"Result: \" + str(acc))\n",
    "    \n",
    "    del images_train\n",
    "    del labels_train\n",
    "    del images_test\n",
    "    del labels_test\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shuffled, y_shuffled = shuffle_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_shuffled, y_shuffled, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_mean = np.mean(X_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess(X_train)\n",
    "X_valid = preprocess(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (5, 5), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adadelta(), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs, \n",
    "          verbose=1,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
